# Chunking

Chunking in AI and machine learning refers to the process of breaking down large datasets, sequences, or text into smaller, manageable pieces or "chunks." This technique is commonly used in natural language processing (NLP) to handle long documents or in tasks requiring processing sequences longer than the model's maximum context length. By dividing data into chunks, models can efficiently process each segment while retaining context across chunks, typically by overlapping or using special tokens to indicate continuation. Chunking improves memory efficiency, reduces computational load, and is essential when working with large datasets or texts that exceed the model's input limitations.

Learn more from the following resources:

- [@article@Chunk large documents for vector search solutions in Azure AI Search](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents)
- [@article@Chunking Strategies for LLM Applications](https://www.pinecone.io/learn/chunking-strategies/)
- [@article@Demystifying Content Chunking In Artificial Intelligence](https://shelf.io/blog/demystifying-content-chunking-in-ai-and-enterprise-knowledge-management/)